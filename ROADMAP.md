# üó∫Ô∏è Implementaƒçn√≠ roadmap

**Projekt:** REST API pro automatizaci k√≥dov√°n√≠ zdravotn√≠ p√©ƒçe  
**Strategie:** Iterativn√≠ v√Ωvoj s MVP ‚Üí Full Product  
**Metodika:** TDD (Red-Green-Refactor) + CI/CD

---

## üìÖ ƒåasov√Ω harmonogram

| F√°ze | Trv√°n√≠ | Obdob√≠ |
|------|--------|--------|
| **0. P≈ô√≠prava prost≈ôed√≠** | 1 t√Ωden | W1 |
| **1. Core Infrastructure** | 2 t√Ωdny | W2-W3 |
| **2. DASTA Parser MVP** | 2 t√Ωdny | W4-W5 |
| **3. Validation Engine** | 3 t√Ωdny | W6-W8 |
| **4. LLM Service** | 2 t√Ωdny | W9-W10 |
| **5. Integration & Testing** | 2 t√Ωdny | W11-W12 |
| **6. Deployment & Launch** | 1 t√Ωden | W13 |

**Celkem:** ~13 t√Ωdn≈Ø (3 mƒõs√≠ce)

---

## üéØ F√°ze 0: P≈ô√≠prava prost≈ôed√≠ (W1)

### C√≠le
- Nastavit v√Ωvojov√© prost≈ôed√≠
- P≈ôipravit CI/CD pipeline
- Vytvo≈ôit z√°kladn√≠ projekt strukturu

### √ökoly

#### 0.1 Project Setup
```bash
# Vytvo≈ôit Python package strukturu
mkdir -p src/coder_api/{api,services,models,repositories,core,utils}
touch src/coder_api/__init__.py

# Poetry/pip setup
poetry init
poetry add fastapi uvicorn sqlalchemy psycopg2-binary redis pydantic-settings
poetry add --group dev pytest pytest-cov black ruff mypy
```

#### 0.2 Docker Compose pro dev
```yaml
# deploy/docker/docker-compose.yml
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: coder_api_dev
      POSTGRES_USER: dev
      POSTGRES_PASSWORD: dev123
    ports:
      - "5432:5432"
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

#### 0.3 GitHub Actions CI
```yaml
# .github/workflows/ci.yml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - run: pytest --cov=src --cov-report=xml
      - run: ruff check src/
      - run: mypy src/
```

#### 0.4 Pre-commit Hooks
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.9
    hooks:
      - id: ruff
        args: [--fix]
  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
```

**V√Ωstupy:**
- ‚úÖ Funkƒçn√≠ `docker-compose up` s PostgreSQL + Redis
- ‚úÖ GitHub Actions pipeline (green)
- ‚úÖ Pre-commit hooks aktivn√≠

**Acceptance Criteria:**
- [ ] `pytest` bƒõ≈æ√≠ (i kdy≈æ bez test≈Ø)
- [ ] `docker-compose up` startuje DB a Redis
- [ ] CI pipeline proch√°z√≠

---

## üîß F√°ze 1: Core Infrastructure (W2-W3)

### C√≠le
- Nastavit FastAPI aplikaci
- P≈ôipravit datab√°zov√© p≈ôipojen√≠
- Implementovat autentizaci
- Vytvo≈ôit monitoring

### √ökoly

#### 1.1 FastAPI Main App (TDD)
**Test first:**
```python
# tests/unit/test_main.py
def test_app_health_endpoint():
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json() == {"status": "healthy"}
```

**Implementace:**
```python
# src/coder_api/main.py
from fastapi import FastAPI

app = FastAPI(
    title="Coder API",
    version="1.0.0",
    docs_url="/docs",
    openapi_url="/openapi.json"
)

@app.get("/health")
async def health_check():
    return {"status": "healthy"}
```

#### 1.2 Config Management
```python
# src/coder_api/config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    DATABASE_URL: str
    REDIS_URL: str
    GEMINI_API_KEY: str
    SECRET_KEY: str
    
    class Config:
        env_file = ".env"

settings = Settings()
```

#### 1.3 Database Setup (Alembic)
```bash
# Alembic init
alembic init migrations

# Prvn√≠ migrace
alembic revision --autogenerate -m "Initial schema"
```

**SQL migrace:**
```sql
-- migrations/versions/001_initial_schema.py
def upgrade():
    op.create_table(
        'ciselnik_mkn10',
        sa.Column('id', sa.Integer, primary_key=True),
        sa.Column('kod', sa.String(10), unique=True),
        sa.Column('nazev', sa.Text),
        sa.Column('metadata', JSONB)
    )
    # ... dal≈°√≠ 3 tabulky
```

#### 1.4 Authentication (OAuth2 + JWT)
**Test:**
```python
# tests/unit/test_security.py
def test_jwt_token_generation():
    token = create_access_token({"sub": "user123"})
    payload = decode_token(token)
    assert payload["sub"] == "user123"
```

**Implementace:**
```python
# src/coder_api/core/security.py
from jose import jwt

def create_access_token(data: dict, expires_delta: timedelta = timedelta(hours=1)):
    to_encode = data.copy()
    expire = datetime.utcnow() + expires_delta
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, settings.SECRET_KEY, algorithm="HS256")
```

#### 1.5 Monitoring Setup
```python
# src/coder_api/core/metrics.py
from prometheus_client import Counter, Histogram, make_asgi_app

request_count = Counter('api_requests_total', 'Total requests', ['endpoint'])
request_duration = Histogram('api_request_duration_seconds', 'Duration')

# Mount Prometheus endpoint
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)
```

**V√Ωstupy:**
- ‚úÖ FastAPI app s `/health`, `/docs`, `/metrics`
- ‚úÖ PostgreSQL connection pool
- ‚úÖ Redis connection
- ‚úÖ JWT autentizace
- ‚úÖ Alembic migrace (4 tabulky)

**Acceptance Criteria:**
- [ ] `curl http://localhost:8000/health` vrac√≠ 200
- [ ] `alembic upgrade head` vytvo≈ô√≠ 4 tabulky
- [ ] JWT token se spr√°vnƒõ generuje a validuje
- [ ] `/metrics` endpoint vrac√≠ Prometheus data

---

## üîç F√°ze 2: DASTA Parser MVP (W4-W5)

### C√≠le
- Implementovat parser KDAVKA.201 form√°tu
- Validovat strukturu souboru
- P≈ôipravit API endpoint pro upload

### √ökoly

#### 2.1 DASTA Parser (TDD)
**Test:**
```python
# tests/unit/test_dasta_parser.py
def test_parse_kdavka_header():
    content = "DASTA:0100:KDAVKA201:202401:12345678:..."
    parser = DASTAParser()
    result = parser.parse_kdavka(content)
    
    assert result.header.version == "0100"
    assert result.header.format == "KDAVKA201"
    assert result.header.period == "202401"

def test_parse_kdavka_records():
    content = """
    DASTA:0100:KDAVKA201:...
    12345678|A001|20240115|Z001|...
    12345678|A002|20240116|Z002|...
    """
    parser = DASTAParser()
    result = parser.parse_kdavka(content)
    
    assert len(result.records) == 2
    assert result.records[0].patient_id == "12345678"
```

**Implementace:**
```python
# src/coder_api/services/dasta_parser.py
from dataclasses import dataclass
from typing import List

@dataclass
class KdavkaHeader:
    version: str
    format: str
    period: str
    provider_id: str

@dataclass
class KdavkaRecord:
    patient_id: str
    record_type: str
    date: str
    diagnosis_code: str

class DASTAParser:
    def parse_kdavka(self, content: str) -> KdavkaModel:
        lines = content.strip().split('\n')
        header = self._parse_header(lines[0])
        records = [self._parse_record(line) for line in lines[1:]]
        return KdavkaModel(header=header, records=records)
    
    def _parse_header(self, line: str) -> KdavkaHeader:
        parts = line.split(':')
        return KdavkaHeader(
            version=parts[1],
            format=parts[2],
            period=parts[3],
            provider_id=parts[4]
        )
    
    def _parse_record(self, line: str) -> KdavkaRecord:
        # Fixed-width parsing podle DASTA spec
        return KdavkaRecord(
            patient_id=line[0:8],
            record_type=line[9:13],
            date=line[14:22],
            diagnosis_code=line[23:28]
        )
```

#### 2.2 API Endpoint (US1)
**Test:**
```python
# tests/integration/test_api_kdavka.py
def test_validate_kdavka_endpoint(client, auth_headers):
    with open("tests/fixtures/valid_kdavka.201", "rb") as f:
        response = client.post(
            "/v1/validate-kdavka",
            files={"file": f},
            headers=auth_headers
        )
    
    assert response.status_code == 200
    data = response.json()
    assert "errors" in data
    assert "recommendations" in data
```

**Implementace:**
```python
# src/coder_api/api/v1/kdavka.py
from fastapi import APIRouter, UploadFile, Depends

router = APIRouter(prefix="/v1", tags=["kdavka"])

@router.post("/validate-kdavka", dependencies=[Depends(get_current_user)])
async def validate_kdavka(
    file: UploadFile,
    parser: DASTAParser = Depends(get_dasta_parser)
):
    content = await file.read()
    content_str = content.decode("utf-8")
    
    # Parse
    kdavka = parser.parse_kdavka(content_str)
    
    # Placeholder validation (Phase 3)
    return {
        "errors": [],
        "recommendations": [],
        "summary": {
            "total_records": len(kdavka.records),
            "parsed_successfully": True
        }
    }
```

**V√Ωstupy:**
- ‚úÖ DASTA Parser s unit testy (‚â•80% coverage)
- ‚úÖ API endpoint `POST /v1/validate-kdavka`
- ‚úÖ OpenAPI spec autogenerovan√°
- ‚úÖ Fixture data (`tests/fixtures/valid_kdavka.201`)

**Acceptance Criteria:**
- [ ] Parser zpracuje vzorovou K-d√°vku z `/docs/example/`
- [ ] API vrac√≠ 200 pro validn√≠ soubor
- [ ] API vrac√≠ 400 pro nevalidn√≠ form√°t
- [ ] Pytest coverage ‚â• 80%

---

## ‚úÖ F√°ze 3: Validation Engine (W6-W8)

### C√≠le
- Naimportovat MKN-10 a SZV ƒç√≠seln√≠ky do DB
- Implementovat validaƒçn√≠ pravidla
- Integrovat Redis cache
- Dokonƒçit US1

### √ökoly

#### 3.1 Import ƒå√≠seln√≠k≈Ø
**Script:**
```python
# scripts/load_codebooks.py
import pandas as pd
from sqlalchemy import create_engine

def load_mkn10_codebook():
    # Parse PDF nebo Excel z /docs/legislativni-docs/
    df = pd.read_csv("mkn10_export.csv")
    engine = create_engine(settings.DATABASE_URL)
    df.to_sql('ciselnik_mkn10', engine, if_exists='append', index=False)
    print(f"Loaded {len(df)} MKN-10 codes")

if __name__ == "__main__":
    load_mkn10_codebook()
```

#### 3.2 Codebook Repository
**Test:**
```python
# tests/unit/test_codebook_repo.py
def test_get_mkn10_code_exists(db_session):
    repo = CodebookRepository(db_session)
    code = repo.get_mkn10_code("A00.0")
    
    assert code is not None
    assert code.kod == "A00.0"
    assert "cholera" in code.nazev.lower()

def test_get_mkn10_code_not_exists(db_session):
    repo = CodebookRepository(db_session)
    code = repo.get_mkn10_code("INVALID")
    
    assert code is None
```

**Implementace:**
```python
# src/coder_api/repositories/codebook_repo.py
from sqlalchemy.orm import Session
from src.coder_api.models.database import CiselnikMKN10

class CodebookRepository:
    def __init__(self, db: Session):
        self.db = db
    
    def get_mkn10_code(self, kod: str) -> CiselnikMKN10 | None:
        return self.db.query(CiselnikMKN10).filter_by(kod=kod).first()
    
    def get_all_mkn10_codes(self) -> list[CiselnikMKN10]:
        return self.db.query(CiselnikMKN10).all()
```

#### 3.3 Cache Service (Redis)
**Test:**
```python
# tests/unit/test_cache_service.py
@pytest.mark.asyncio
async def test_cache_hit(redis_client):
    cache = CacheService(redis_client)
    
    # Set
    await cache.set_mkn10_code("A00.0", {"nazev": "Cholera"})
    
    # Get (hit)
    result = await cache.get_mkn10_code("A00.0")
    assert result["nazev"] == "Cholera"

@pytest.mark.asyncio
async def test_cache_miss(redis_client):
    cache = CacheService(redis_client)
    result = await cache.get_mkn10_code("NONEXISTENT")
    assert result is None
```

**Implementace:**
```python
# src/coder_api/services/cache_service.py
import json
from redis.asyncio import Redis

class CacheService:
    def __init__(self, redis: Redis):
        self.redis = redis
        self.ttl = 86400  # 24 hours
    
    async def get_mkn10_code(self, kod: str) -> dict | None:
        key = f"mkn10:{kod}"
        cached = await self.redis.get(key)
        return json.loads(cached) if cached else None
    
    async def set_mkn10_code(self, kod: str, data: dict):
        key = f"mkn10:{kod}"
        await self.redis.setex(key, self.ttl, json.dumps(data))
```

#### 3.4 Validation Engine
**Test:**
```python
# tests/unit/test_validation_engine.py
def test_validate_mkn10_code_valid():
    engine = ValidationEngine(codebook_repo, cache)
    result = engine.validate_mkn10_code("A00.0")
    
    assert result.is_valid == True
    assert len(result.errors) == 0

def test_validate_mkn10_code_invalid():
    engine = ValidationEngine(codebook_repo, cache)
    result = engine.validate_mkn10_code("INVALID")
    
    assert result.is_valid == False
    assert "K√≥d MKN-10 neexistuje" in result.errors[0].message
```

**Implementace:**
```python
# src/coder_api/services/validation_engine.py
from typing import List
from dataclasses import dataclass

@dataclass
class ValidationError:
    code: str
    message: str
    severity: str  # 'error' | 'warning'
    line_number: int

@dataclass
class ValidationResult:
    is_valid: bool
    errors: List[ValidationError]
    recommendations: List[str]

class ValidationEngine:
    def __init__(self, codebook_repo: CodebookRepository, cache: CacheService):
        self.codebook_repo = codebook_repo
        self.cache = cache
    
    async def validate_kdavka(self, kdavka: KdavkaModel) -> ValidationResult:
        errors = []
        
        for idx, record in enumerate(kdavka.records):
            # 1. Validate MKN-10 code exists
            if not await self._validate_mkn10_code(record.diagnosis_code):
                errors.append(ValidationError(
                    code="INVALID_MKN10",
                    message=f"K√≥d MKN-10 '{record.diagnosis_code}' neexistuje",
                    severity="error",
                    line_number=idx + 2  # Header is line 1
                ))
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            recommendations=self._generate_recommendations(errors)
        )
    
    async def _validate_mkn10_code(self, kod: str) -> bool:
        # Try cache first
        cached = await self.cache.get_mkn10_code(kod)
        if cached:
            return True
        
        # Fallback to DB
        code = self.codebook_repo.get_mkn10_code(kod)
        if code:
            await self.cache.set_mkn10_code(kod, code.to_dict())
            return True
        
        return False
```

#### 3.5 Complete US1 Integration
```python
# src/coder_api/api/v1/kdavka.py (update)
@router.post("/validate-kdavka")
async def validate_kdavka(
    file: UploadFile,
    parser: DASTAParser = Depends(get_dasta_parser),
    validator: ValidationEngine = Depends(get_validation_engine)
):
    content = await file.read()
    content_str = content.decode("utf-8")
    
    # Parse
    kdavka = parser.parse_kdavka(content_str)
    
    # Validate
    result = await validator.validate_kdavka(kdavka)
    
    return {
        "is_valid": result.is_valid,
        "errors": [e.__dict__ for e in result.errors],
        "recommendations": result.recommendations,
        "summary": {
            "total_records": len(kdavka.records),
            "error_count": len(result.errors)
        }
    }
```

**V√Ωstupy:**
- ‚úÖ MKN-10 ƒç√≠seln√≠k v PostgreSQL (~14,000 z√°znam≈Ø)
- ‚úÖ SZV ƒç√≠seln√≠k v PostgreSQL
- ‚úÖ Redis cache pro ƒç√≠seln√≠ky
- ‚úÖ Validation Engine s 5+ pravidly
- ‚úÖ US1 plnƒõ funkƒçn√≠

**Acceptance Criteria:**
- [ ] `scripts/load_codebooks.py` naimportuje MKN-10
- [ ] Cache hit ratio ‚â• 90% (po warm-up)
- [ ] Validace detekuje neplatn√© k√≥dy
- [ ] Response time <200ms (bez LLM)
- [ ] E2E test pro US1 proch√°z√≠

---

## ü§ñ F√°ze 4: LLM Service (W9-W10)

### C√≠le
- Integrovat Google Gemini API
- Implementovat prompt engineering
- Dokonƒçit US2
- Performance testing

### √ökoly

#### 4.1 LLM Service
**Test:**
```python
# tests/unit/test_llm_service.py
@pytest.mark.asyncio
async def test_code_clinical_event(mock_gemini_api):
    service = LLMService(api_key="test_key")
    
    text = "Pacient s akutn√≠ bolest√≠ v krku, diagnostikov√°na ang√≠na"
    result = await service.code_clinical_event(text)
    
    assert len(result.codes) > 0
    assert result.codes[0].code.startswith("J")  # MKN-10 J kategorie
    assert result.codes[0].confidence > 0.7
```

**Implementace:**
```python
# src/coder_api/services/llm_service.py
import google.generativeai as genai
from typing import List

@dataclass
class CodingResult:
    codes: List[dict]  # [{"code": "J03.9", "confidence": 0.85, "reasoning": "..."}]
    raw_response: str

class LLMService:
    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-pro')
    
    async def code_clinical_event(self, clinical_text: str) -> CodingResult:
        prompt = self._build_prompt(clinical_text)
        
        response = await self.model.generate_content_async(
            prompt,
            generation_config={"temperature": 0.2, "max_output_tokens": 500}
        )
        
        return self._parse_response(response.text)
    
    def _build_prompt(self, text: str) -> str:
        return f"""
        Jsi expert na k√≥dov√°n√≠ zdravotn√≠ch ud√°lost√≠ podle MKN-10.
        
        Klinick√Ω text: "{text}"
        
        √ökol: Identifikuj hlavn√≠ diagn√≥zu a p≈ôi≈ôaƒè MKN-10 k√≥d.
        
        Form√°t odpovƒõdi (JSON):
        {{
          "codes": [
            {{
              "code": "A00.0",
              "confidence": 0.85,
              "reasoning": "Vysvƒõtlen√≠..."
            }}
          ]
        }}
        """
    
    def _parse_response(self, raw_text: str) -> CodingResult:
        # Parse JSON z LLM response
        data = json.loads(raw_text)
        return CodingResult(codes=data["codes"], raw_response=raw_text)
```

#### 4.2 API Endpoint US2
**Test:**
```python
# tests/integration/test_api_coding.py
def test_code_event_endpoint(client, auth_headers):
    response = client.post(
        "/v1/code-event",
        json={"clinical_text": "Pacient s diabetem 2. typu"},
        headers=auth_headers
    )
    
    assert response.status_code == 200
    data = response.json()
    assert len(data["codes"]) > 0
    assert data["codes"][0]["code"].startswith("E11")  # Diabetes type 2
```

**Implementace:**
```python
# src/coder_api/api/v1/coding.py
from fastapi import APIRouter, Depends
from pydantic import BaseModel

router = APIRouter(prefix="/v1", tags=["coding"])

class CodingRequest(BaseModel):
    clinical_text: str

@router.post("/code-event")
async def code_event(
    request: CodingRequest,
    llm_service: LLMService = Depends(get_llm_service),
    validator: ValidationEngine = Depends(get_validation_engine)
):
    # LLM coding
    result = await llm_service.code_clinical_event(request.clinical_text)
    
    # Validate generated codes
    for code_entry in result.codes:
        is_valid = await validator._validate_mkn10_code(code_entry["code"])
        code_entry["is_valid"] = is_valid
    
    return {
        "codes": result.codes,
        "processing_time_ms": 2500  # TODO: actual timing
    }
```

#### 4.3 Performance Testing
```python
# tests/performance/test_load.py
from locust import HttpUser, task, between

class CoderAPIUser(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def validate_kdavka(self):
        with open("tests/fixtures/valid_kdavka.201", "rb") as f:
            self.client.post("/v1/validate-kdavka", files={"file": f})
    
    @task
    def code_event(self):
        self.client.post("/v1/code-event", json={
            "clinical_text": "Pacient s hypertenz√≠"
        })
```

**Run:**
```bash
locust -f tests/performance/test_load.py --host=http://localhost:8000
```

**V√Ωstupy:**
- ‚úÖ Gemini API integrace
- ‚úÖ Prompt template optimalizovan√Ω
- ‚úÖ US2 endpoint funkƒçn√≠
- ‚úÖ Load test v√Ωsledky (p95 < 3s)

**Acceptance Criteria:**
- [ ] LLM generuje relevantn√≠ k√≥dy (manu√°ln√≠ review 10 vzork≈Ø)
- [ ] Confidence score ‚â• 0.7 pro p≈ôijet√≠
- [ ] Validace ovƒõ≈ôuje generovan√© k√≥dy
- [ ] p95 latency < 3s (US2)
- [ ] p95 latency < 200ms (US1)

---

## üîó F√°ze 5: Integration & Testing (W11-W12)

### C√≠le
- E2E testy pro oba user stories
- Security testing
- Dokumentace API
- Performance tuning

### √ökoly

#### 5.1 E2E Tests
```python
# tests/e2e/test_full_workflow.py
def test_full_validation_workflow(client, auth_token):
    # 1. Upload K-d√°vka
    with open("tests/fixtures/real_kdavka.201", "rb") as f:
        response = client.post(
            "/v1/validate-kdavka",
            files={"file": f},
            headers={"Authorization": f"Bearer {auth_token}"}
        )
    
    assert response.status_code == 200
    result = response.json()
    
    # 2. Verify errors detected
    assert len(result["errors"]) > 0
    
    # 3. Check audit log
    audit_response = client.get(
        "/v1/audit-logs",
        params={"endpoint": "/v1/validate-kdavka"},
        headers={"Authorization": f"Bearer {auth_token}"}
    )
    assert audit_response.status_code == 200
```

#### 5.2 Security Testing
```bash
# OWASP ZAP scan
docker run -t owasp/zap2docker-stable zap-baseline.py -t http://localhost:8000

# Bandit (Python security linter)
bandit -r src/

# Safety (dependency check)
safety check --json
```

#### 5.3 API Documentation
```python
# src/coder_api/main.py (update)
app = FastAPI(
    title="Coder API",
    version="1.0.0",
    description="""
    REST API pro automatizaci k√≥dov√°n√≠ zdravotn√≠ p√©ƒçe podle MKN-10.
    
    ## Features
    - **US1:** Validace K-d√°vky podle VZP pravidel
    - **US2:** LLM k√≥dov√°n√≠ klinick√©ho textu
    
    ## Authentication
    OAuth2 + JWT tokens (1h TTL)
    """,
    contact={
        "name": "API Support",
        "email": "support@example.com"
    }
)
```

#### 5.4 Performance Tuning
**Database:**
```sql
-- Add indexes for hot queries
CREATE INDEX idx_mkn10_kod_trgm ON ciselnik_mkn10 USING gin(kod gin_trgm_ops);
CREATE INDEX idx_audit_endpoint ON audit_logs(endpoint, created_at);

-- Vacuum analyze
VACUUM ANALYZE;
```

**Redis:**
```python
# Warm-up cache on startup
async def warmup_cache():
    all_codes = codebook_repo.get_all_mkn10_codes()
    for code in all_codes[:1000]:  # Top 1000 codes
        await cache.set_mkn10_code(code.kod, code.to_dict())
```

**V√Ωstupy:**
- ‚úÖ E2E test suite (10+ scenarios)
- ‚úÖ Security scan report
- ‚úÖ OpenAPI spec v3.1 (auto-generated)
- ‚úÖ Performance baseline (p95 < 200ms/3s)

**Acceptance Criteria:**
- [ ] E2E testy pokr√Ωvaj√≠ happy path + error cases
- [ ] Zero high/critical security findings
- [ ] OpenAPI spec validn√≠ (Swagger Validator)
- [ ] Load test: 100 req/s sustained (10 min)

---

## üöÄ F√°ze 6: Deployment & Launch (W13)

### C√≠le
- Nasadit do Kubernetes
- Nastavit monitoring (Prometheus + Grafana)
- Launch checklist

### √ökoly

#### 6.1 Kubernetes Deployment
```bash
# Build Docker image
docker build -t coder-api:v1.0.0 -f deploy/docker/Dockerfile .

# Push to registry
docker tag coder-api:v1.0.0 registry.example.com/coder-api:v1.0.0
docker push registry.example.com/coder-api:v1.0.0

# Apply K8s manifests
kubectl apply -f deploy/k8s/namespace.yaml
kubectl apply -f deploy/k8s/secrets.yaml
kubectl apply -f deploy/k8s/postgres-statefulset.yaml
kubectl apply -f deploy/k8s/redis-deployment.yaml
kubectl apply -f deploy/k8s/api-deployment.yaml
kubectl apply -f deploy/k8s/ingress.yaml
```

#### 6.2 Monitoring Setup
```bash
# Prometheus + Grafana via Helm
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack

# Import dashboard
kubectl apply -f monitoring/grafana-dashboards/api-performance.json
```

#### 6.3 Launch Checklist

**Pre-launch:**
- [ ] All tests passing (unit + integration + e2e)
- [ ] Security scan clean
- [ ] Load test passed (100 req/s)
- [ ] Database backups configured
- [ ] SSL certificates valid
- [ ] Environment variables set (production)
- [ ] Rollback plan documented

**Launch:**
- [ ] Deploy to production K8s cluster
- [ ] Smoke test (health endpoint)
- [ ] Monitor logs (ELK Stack)
- [ ] Monitor metrics (Grafana dashboard)
- [ ] Alert rules active (Alertmanager)

**Post-launch:**
- [ ] Announce to stakeholders
- [ ] Update documentation (URL, API keys)
- [ ] Monitor error rate (first 24h)
- [ ] Collect user feedback

**V√Ωstupy:**
- ‚úÖ Production deployment (K8s)
- ‚úÖ Monitoring dashboards live
- ‚úÖ Launch report

---

## üìä Metriky √∫spƒõchu

| Metrika | C√≠l | Mƒõ≈ôen√≠ |
|---------|-----|--------|
| **Test Coverage** | ‚â•80% | `pytest --cov` |
| **US1 Latency (p95)** | <200ms | Locust report |
| **US2 Latency (p95)** | <3s | Locust report |
| **Cache Hit Ratio** | ‚â•90% | Redis INFO stats |
| **Error Rate** | <1% | Prometheus `api_errors_total` |
| **Uptime (SLA)** | 99.5% | Uptime monitor (30 days) |
| **Security Findings** | 0 critical | OWASP ZAP + Bandit |

---

## üîÑ Iteraƒçn√≠ v√Ωvoj

Po √∫spƒõ≈°n√©m launchi (W13):

**F√°ze 7: Optimalizace (W14-W16)**
- [ ] ML model fine-tuning (Gemini)
- [ ] Advanced validation rules
- [ ] Batch processing API
- [ ] Reporting dashboard

**F√°ze 8: ≈†k√°lov√°n√≠ (W17-W20)**
- [ ] Multi-region deployment
- [ ] CDN integration
- [ ] Auto-scaling policies
- [ ] Cost optimization

---

## üéØ Kritick√© cesty (dependencies)

```
F√°ze 0 ‚Üí F√°ze 1 ‚Üí F√°ze 2 ‚Üí F√°ze 3 ‚Üí F√°ze 5 ‚Üí F√°ze 6
                              ‚Üì
                         F√°ze 4 ‚îÄ‚îÄ‚Üí F√°ze 5
```

**Blocking dependencies:**
- F√°ze 3 z√°vis√≠ na F√°zi 2 (DASTA Parser)
- F√°ze 4 m≈Ø≈æe bƒõ≈æet paralelnƒõ s F√°z√≠ 3
- F√°ze 5 vy≈æaduje dokonƒçen√≠ F√°z√≠ 3 + 4
- F√°ze 6 vy≈æaduje dokonƒçen√≠ F√°ze 5

---

## ‚úÖ Next Steps

1. **Schv√°lit roadmap** s t√Ωmem
2. **Alokovat resources** (dev + QA)
3. **Kickoff meeting** (W1)
4. **Weekly sprint reviews**

**Status:** üü° Draft (ƒçek√° na schv√°len√≠)
